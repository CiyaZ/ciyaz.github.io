# scrapy命令行工具详解

scrapy提供了一个命令行工具，上一节中，在创建项目时和启动Spider时我们已经用到了。这里我们来详细了解一下，这个命令行工具都有哪些功能。

当然，scrapy命令行工具就像其他Linux命令一样，有很多功能，很多选项。我只是挑一部分有用的，记录一下。如果以后遇到了特殊的情况，还是应该参考官方文档。

## 创建项目

```
scrapy startproject myproject
```

在当前目录创建一个叫做`myproject`的scrapy项目。推荐使用这种方式创建项目，scrapy会帮我们创建很多文件，我们直接修改一下就能使用了。

## 创建一个Spider

```
scrapy genspider <name> <domain>
```

自动创建一个Spider类文件，`name`是Spider的名字（name类属性），`domain`是Spider的`allowed_domains`和`start_urls`属性。

例子：
```
scrapy genspider TestSpider www.zhihu.com
```

生成的Spider
```python
# -*- coding: utf-8 -*-
import scrapy


class TestspiderSpider(scrapy.Spider):
    name = 'TestSpider'
    allowed_domains = ['www.zhihu.com']
    start_urls = ['http://www.zhihu.com/']

    def parse(self, response):
        pass
```

当然，其实我们手写也并不麻烦，只是使用命令创建Spider更方便一点。

## 执行Spider

```
scrapy crawl <spider>
```

上一节我们已经使用过了，写好Spider后，就可以使用这个命令执行了。不要忘了上一节我们使用的很好用的`-o`选项，它可以简单的将数据持久化到文件。

## 显示所有爬虫

```
scrapy list
```

这个命令显示我们项目里所有的Spider。

## 模拟爬虫执行

```
scrapy fetch [url]
```

编写爬虫之前，我们可以用`scrapy fetch`先试着请求我们要爬取的URL，因为有些网站会检查`refer`，`user-agent`之类的HTTP报头。fetch会根据`settings.py`的配置对指定URL发一个请求，我们可以观察一下settings里指定的配置能否骗过我们要爬取的网站。

## scrapy shell

```
scrapy shell [url]
```

我们上一节已经用过了，上一节在`scrapy shell`中，我们请求了一个URL，在返回的html中测试css选择器。

利用该功能调试采集逻辑，是爬虫程序开发中，十分重要的一步。
